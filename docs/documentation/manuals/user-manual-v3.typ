// Voice Notepad User Manual v3
// Version 1.9.11 - December 2025

// Define colors
#let theme-blue = rgb(67, 97, 238)
#let pipeline-local = rgb(34, 197, 94)    // Green for local processing
#let pipeline-cloud = rgb(99, 102, 241)   // Purple for cloud processing
#let linux-yellow = rgb(252, 198, 36)
#let debian-red = rgb(168, 29, 51)
#let wayland-teal = rgb(0, 150, 136)
#let gemini-blue = rgb(66, 133, 244)
#let warning-orange = rgb(245, 158, 11)

#set document(
  title: "Voice Notepad User Manual v3",
  author: "Daniel Rosehill",
)

#set page(
  paper: "a4",
  margin: (x: 2cm, y: 2.5cm),
  header: context {
    if counter(page).get().first() > 1 [
      #set text(9pt, fill: rgb(102, 102, 102))
      #h(1fr) Voice Notepad User Manual v3
    ]
  },
  footer: context {
    if counter(page).get().first() > 1 [
      #set text(9pt, fill: rgb(102, 102, 102))
      #link("https://danielrosehill.com")[danielrosehill.com]
      #h(1fr)
      github.com/danielrosehill/Voice-Notepad
      #h(1fr)
      #text(weight: "bold", size: 10pt)[Page #counter(page).display()]
    ]
  },
)

#set text(
  font: "IBM Plex Sans",
  size: 11pt,
)

#set heading(numbering: "1.1")
#set block(breakable: true)

#show heading.where(level: 1): it => {
  set text(24pt, weight: "bold", fill: rgb(26, 26, 46))
  pagebreak(weak: true)
  block(below: 1em)[
    #it.body
    #v(0.3em)
    #line(length: 100%, stroke: 2pt + theme-blue)
  ]
}

#show heading.where(level: 2): it => {
  set text(16pt, weight: "bold", fill: rgb(26, 26, 46))
  block(above: 1.5em, below: 0.8em, breakable: false)[
    #it
    #v(0.2em)
    #line(length: 100%, stroke: 0.5pt + rgb(204, 204, 204))
  ]
}

#show heading.where(level: 3): it => {
  set text(13pt, weight: "bold", fill: rgb(45, 52, 54))
  block(above: 1.2em, below: 0.6em)[#it]
}

#show link: it => {
  set text(fill: theme-blue)
  underline(it)
}

#show figure: it => block(breakable: false)[#it]

// Badge helper
#let badge(label, bg-color, text-color: white) = {
  box(
    fill: bg-color,
    inset: (x: 10pt, y: 5pt),
    radius: 4pt,
    text(weight: "bold", size: 10pt, fill: text-color)[#label]
  )
}

// Pipeline stage box
#let stage-box(title, color, content) = {
  block(
    fill: color.lighten(90%),
    stroke: 2pt + color,
    radius: 8pt,
    inset: 12pt,
    width: 100%,
  )[
    #text(weight: "bold", fill: color)[#title]
    #v(0.3em)
    #content
  ]
}

// Info box
#let info-box(content) = {
  block(
    fill: theme-blue.lighten(90%),
    stroke: 1pt + theme-blue,
    radius: 6pt,
    inset: 12pt,
    width: 100%,
  )[#content]
}

// Warning box
#let warning-box(content) = {
  block(
    fill: warning-orange.lighten(85%),
    stroke: 1pt + warning-orange,
    radius: 6pt,
    inset: 12pt,
    width: 100%,
  )[#content]
}

// Flow diagram helper - creates a box with text
#let flow-box(label, color, width: 100pt) = {
  box(
    fill: color.lighten(80%),
    stroke: 2pt + color,
    radius: 6pt,
    inset: 8pt,
    width: width,
  )[
    #align(center)[
      #text(size: 9pt, weight: "bold", fill: color.darken(20%))[#label]
    ]
  ]
}

// Arrow helper
#let arrow = text(size: 14pt, weight: "bold", fill: rgb(150, 150, 150))[ → ]

// Title Page
#align(center)[
  #v(2cm)
  #text(42pt, weight: "bold", fill: rgb(26, 26, 46))[Voice Notepad]
  #v(0.3cm)
  #text(18pt, fill: rgb(102, 102, 102))[User Manual v3]
  #v(0.5cm)
  #text(14pt)[Version 1.9.11 · December 2025]

  #v(1cm)

  // Pipeline badges
  #text(11pt, weight: "bold", fill: rgb(102, 102, 102))[DUAL-PIPELINE ARCHITECTURE]
  #v(0.3cm)
  #badge("Local Preprocessing", pipeline-local)
  #h(0.5cm)
  #text(size: 16pt)[+]
  #h(0.5cm)
  #badge("Cloud Transcription", pipeline-cloud)

  #v(1cm)

  // Platform badges
  #text(11pt, weight: "bold", fill: rgb(102, 102, 102))[PLATFORMS]
  #v(0.3cm)
  #badge("Linux", linux-yellow, text-color: black)
  #h(0.3cm)
  #badge("Debian/Ubuntu", debian-red)
  #h(0.3cm)
  #badge("Wayland", wayland-teal)

  #v(1.5cm)

  #text(12pt)[
    *Author:* Daniel Rosehill \
    *Repository:* #link("https://github.com/danielrosehill/Voice-Notepad")[github.com/danielrosehill/Voice-Notepad] \
    *License:* MIT
  ]

]

#pagebreak()

// Table of Contents
#outline(
  title: [Table of Contents],
  indent: 1.5em,
  depth: 3,
)

#pagebreak()

// AI-Human Co-Authorship Page
#align(center)[
  #v(3cm)
  #text(28pt, weight: "bold", fill: rgb(26, 26, 46))[AI-Human Co-Authorship]
  #v(0.5em)
  #line(length: 40%, stroke: 2pt + theme-blue)
  #v(2cm)
]

#block(
  fill: rgb(248, 249, 250),
  stroke: 1pt + rgb(200, 200, 200),
  radius: 12pt,
  inset: 24pt,
  width: 100%,
)[
  #text(12pt)[
    This software was developed through *AI-human collaboration*.

    #v(1em)

    The code was generated by *Claude Opus 4.5* and other Anthropic models under my direction and supervision. I designed the architecture, specified requirements, reviewed outputs, and guided the implementation. Claude wrote the code.

    #v(1em)

    This represents an experimental approach to software development. I believe it produces high-quality results when the human provides clear direction, domain expertise, and ongoing oversight.

    #v(1em)

    The core design philosophy—*audio multimodal first*—came from my experience with traditional ASR-then-LLM workflows and recognizing that multimodal models could do both in a single pass. The iterative development process has been guided by my own daily use of the application.

    #v(1.5em)

    #align(right)[
      #text(style: "italic")[— Daniel Rosehill, December 2025]
    ]
  ]
]

#pagebreak()

= Introduction

Voice Notepad is an *audio multimodal first* desktop application for voice transcription. The core design philosophy is to send audio directly to multimodal AI models that can "hear" and process audio alongside text instructions—rather than traditional ASR-then-LLM approaches.

== Design Philosophy

This is an *experimental, iterative software design process* based on my own need for a better transcription tool. In the first few weeks of use, the approach has been validated with:

- *Over 2,000 transcriptions* processed
- *More than 1 million characters* of output
- Excellent accuracy and formatting results
- API costs of just a few dollars total

The key insight is that multimodal models can do both transcription and cleanup in a single pass, while also "hearing" tone, emphasis, and verbal commands that get lost in text-only processing.

== What Makes This Different

#grid(
  columns: (1fr, 1fr),
  gutter: 16pt,
  [
    *Traditional Approach*
    #v(0.5em)
    #block(
      fill: rgb(254, 226, 226),
      stroke: 1pt + rgb(220, 38, 38),
      radius: 6pt,
      inset: 12pt,
    )[
      1. Record audio
      2. Send to ASR (Whisper, etc.)
      3. Get raw transcript
      4. Send text to LLM for cleanup
      5. Get formatted output

      *Two API calls, higher cost*
    ]
  ],
  [
    *Voice Notepad Approach*
    #v(0.5em)
    #block(
      fill: rgb(220, 252, 231),
      stroke: 1pt + rgb(34, 197, 94),
      radius: 6pt,
      inset: 12pt,
    )[
      1. Record audio
      2. Local preprocessing (VAD + AGC)
      3. Send audio + prompt to Gemini
      4. Get formatted output

      *Single API call, lower cost* \
      *AI "hears" your voice*
    ]
  ],
)

== Key Features

#grid(
  columns: (1fr, 1fr),
  gutter: 12pt,
  [
    - Dual-pipeline processing
    - Voice Activity Detection (VAD)
    - Automatic Gain Control (AGC)
    - Single-pass multimodal transcription
    - Layered prompt concatenation
    - Configurable hotkeys (F13-F24)
  ],
  [
    - Text injection (auto-paste)
    - Audio feedback (beeps or TTS)
    - Transcript history with search
    - Usage analytics and charts
    - Cost tracking
    - Statistics export
  ],
)

= The Dual-Pipeline Architecture

Voice Notepad's effectiveness comes from combining local preprocessing with cloud-based multimodal transcription.

== Pipeline Overview

#align(center)[
  #block(
    fill: rgb(248, 249, 250),
    stroke: 1pt + rgb(200, 200, 200),
    radius: 8pt,
    inset: 20pt,
    width: 95%,
  )[
    #text(10pt, weight: "bold", fill: pipeline-local)[STAGE 1: LOCAL PREPROCESSING]
    #v(0.8em)

    #align(center)[
      #flow-box("Recording\n48kHz", pipeline-local, width: 80pt)
      #arrow
      #flow-box("AGC\nNormalize", pipeline-local, width: 80pt)
      #arrow
      #flow-box("VAD\nSilence removal", pipeline-local, width: 90pt)
      #arrow
      #flow-box("Compress\n16kHz mono", pipeline-local, width: 85pt)
    ]

    #v(1.2em)
    #align(center)[
      #text(size: 20pt, fill: rgb(150, 150, 150))[⬇]
    ]
    #v(0.8em)

    #text(10pt, weight: "bold", fill: pipeline-cloud)[STAGE 2: CLOUD TRANSCRIPTION]
    #v(0.8em)

    #align(center)[
      #flow-box("Prompt\nConcatenation", pipeline-cloud, width: 90pt)
      #arrow
      #flow-box("Gemini API\nAudio + Prompt", pipeline-cloud, width: 95pt)
      #arrow
      #flow-box("Output\nFormatted text", pipeline-cloud, width: 85pt)
    ]
  ]
]

#v(1em)

The combination achieves:
- *Cost reduction*: VAD removes 30-80% of audio (silence/pauses)
- *Quality improvement*: AGC ensures consistent input levels
- *Precise formatting*: Layered prompts produce exactly the output you need
- *Single API call*: Multimodal models transcribe and format in one pass

== Stage 1: Local Preprocessing

The local pipeline processes audio on your machine before any cloud upload.

#stage-box("1.1 Recording (PyAudio)", pipeline-local)[
  - Captures at device's native sample rate (typically 48kHz)
  - 16-bit PCM, mono audio
  - Automatic sample rate negotiation
  - Handles microphone disconnection gracefully
]

#v(0.5em)

#stage-box("1.2 Automatic Gain Control (AGC)", pipeline-local)[
  Normalizes audio levels for consistent transcription accuracy.

  #table(
    columns: (auto, auto, 1fr),
    inset: 8pt,
    fill: (x, y) => if y == 0 { pipeline-local.lighten(70%) } else { white },
    table.header([*Parameter*], [*Value*], [*Purpose*]),
    [Target peak], [-3 dBFS], [Optimal level with headroom],
    [Min threshold], [-40 dBFS], [Skip if quieter than noise floor],
    [Max gain], [+20 dB], [Prevent over-amplification],
  )

  *Behavior*: Only boosts quiet audio—never attenuates loud audio.
]

#v(0.5em)

#stage-box("1.3 Voice Activity Detection (VAD)", pipeline-local)[
  Removes silence using TEN VAD before API upload.

  #table(
    columns: (auto, auto, 1fr),
    inset: 8pt,
    fill: (x, y) => if y == 0 { pipeline-local.lighten(70%) } else { white },
    table.header([*Parameter*], [*Value*], [*Purpose*]),
    [Sample rate], [16kHz], [Required by TEN VAD],
    [Hop size], [256 samples], [~16ms analysis windows],
    [Threshold], [0.5], [Speech probability cutoff],
    [Min speech], [250ms], [Ignore very short sounds],
    [Padding], [30ms], [Buffer around speech],
  )

  *Typical reduction*: 30-80% depending on speaking pattern.
]

#v(0.5em)

#stage-box("1.4 Compression", pipeline-local)[
  - Downsampled to 16kHz mono (matches Gemini's internal format)
  - Converted to WAV for API upload
  - ~66% smaller than 48kHz stereo input
]

== Stage 2: Cloud Transcription

#stage-box("2.1 Prompt Concatenation", pipeline-cloud)[
  Instructions are built from multiple layers:

  *Foundation Layer* (always applied):
  - Remove filler words (um, uh, like, you know)
  - Add punctuation and paragraph breaks
  - Follow verbal commands ("scratch that", "new paragraph")
  - Fix grammar and spelling

  *Format Layer* (based on preset):
  - Email, meeting notes, bullet points, documentation, etc.

  *Style Layer*:
  - Formality: casual, neutral, professional
  - Verbosity: none to maximum reduction

  *Personalization*:
  - Email signatures injected for email format
]

#v(0.5em)

#stage-box("2.2 Multimodal API Submission", pipeline-cloud)[
  - Audio is base64-encoded with concatenated prompt
  - Gemini processes audio + text instructions together
  - The AI "hears" tone, emphasis, and verbal commands
  - Returns formatted text in a single API call
]

= Installation

Voice Notepad requires a two-stage installation: system dependencies first, then the application.

== Stage 1: System Dependencies

Install required system packages on Ubuntu/Debian:

```bash
sudo apt install python3 python3-venv ffmpeg portaudio19-dev libc++1
```

#table(
  columns: (auto, 1fr),
  inset: 10pt,
  fill: (x, y) => if y == 0 { theme-blue } else if calc.odd(y) { rgb(248, 249, 250) } else { white },
  table.header(text(fill: white)[*Package*], text(fill: white)[*Purpose*]),
  [`python3`, `python3-venv`], [Python runtime and virtual environments],
  [`ffmpeg`], [Audio format conversion and compression],
  [`portaudio19-dev`], [PyAudio recording library headers],
  [`libc++1`], [Required by TEN VAD for voice activity detection],
)

== Stage 2: Application Installation

=== Option A: Debian Package (Recommended)

```bash
sudo apt install ./voice-notepad_1.9.11_amd64.deb
```

=== Option B: AppImage

```bash
chmod +x Voice_Notepad-1.9.11-x86_64.AppImage
./Voice_Notepad-1.9.11-x86_64.AppImage
```

=== Option C: From Source

```bash
git clone https://github.com/danielrosehill/Voice-Notepad.git
cd Voice-Notepad
./run.sh
```

= Hardware Recommendations

Voice Notepad is designed to work with dedicated hardware for hands-free operation.

== Design Rationale: F13-F24 Keys

The application uses *F13 through F24* as the default hotkey range for an important reason: *these keys are defined in Linux but virtually never used by applications*.

This design choice ensures:
- *No conflicts* with user-level programs (browsers, editors, etc.)
- *No conflicts* with desktop environment shortcuts
- *System-level interception* without interfering with normal keyboard use
- *Clean separation* between transcription controls and regular typing

Most standard keyboards only have F1-F12. To use Voice Notepad's hotkeys, you need either a keyboard with extended function keys or a separate input device mapped to F13-F24.

== Recommended Hardware

=== Simple: USB HID Button (~\$5)

#info-box[
  A single USB HID programmable button (available on AliExpress for ~\$5) provides excellent *push-to-talk (PTT)* functionality.

  - Map the button to *F15* (Toggle Recording)
  - Press to start recording, press again to transcribe
  - Simple, reliable, inexpensive
  - No software required after initial mapping
]

=== Full Setup: Macro Pad

For power users, a USB macro pad with 6+ keys enables the full hotkey workflow:

#table(
  columns: (auto, auto, 1fr),
  inset: 10pt,
  fill: (x, y) => if y == 0 { theme-blue } else if calc.odd(y) { rgb(248, 249, 250) } else { white },
  table.header(text(fill: white)[*Button*], text(fill: white)[*Maps To*], text(fill: white)[*Function*]),
  [1], [F15], [Toggle (start/stop and transcribe)],
  [2], [F16], [Tap Toggle (start/stop and cache)],
  [3], [F17], [Transcribe cached audio],
  [4], [F18], [Clear recording],
  [5], [F19], [Append to cache],
  [6], [F20], [Pause/resume],
)

The number of configurable hotkeys (6 functions) is specifically designed to match typical macro pad layouts.

=== Alternative: Foot Pedal

USB foot pedals are popular for hands-free transcription:
- Keep hands on keyboard while controlling recording
- Map pedal buttons to F15/F17/F18
- Available from ~\$15-30

== Setting Up Key Mapping

Use *Input Remapper* on Linux to map your device:

```bash
sudo apt install input-remapper
```

1. Open Input Remapper
2. Select your USB device
3. Click "Record" and press the button
4. Set output to `KEY_F15` (or desired F-key)
5. Click Apply and enable Autoload

= Configurable Hotkeys

== Default Mappings

#table(
  columns: (auto, auto, 1fr),
  inset: 10pt,
  fill: (x, y) => if y == 0 { theme-blue } else if calc.odd(y) { rgb(248, 249, 250) } else { white },
  table.header(text(fill: white)[*Key*], text(fill: white)[*Function*], text(fill: white)[*Description*]),
  [F15], [Toggle], [Start recording, or stop and transcribe immediately],
  [F16], [Tap Toggle], [Start recording, or stop and cache],
  [F17], [Transcribe], [Transcribe cached audio],
  [F18], [Clear], [Delete recording and clear cache],
  [F19], [Append], [Start recording that appends to cache],
  [F20], [Pause], [Pause/resume current recording],
)

== Configuration

Go to *Settings → Hotkeys* to customize mappings. Each function can be assigned any key from F13-F24, or disabled entirely.

== Technical Notes

- On Linux/Wayland: Hotkeys work via evdev (reads directly from input devices)
- Requires user to be in the `input` group: `sudo usermod -aG input $USER`
- Falls back to pynput/X11 on non-Linux systems

= Audio Feedback

Voice Notepad provides audio notifications for recording events.

== Modes

Configure in *Settings → Behavior → Audio feedback*:

#table(
  columns: (auto, 1fr),
  inset: 10pt,
  fill: (x, y) => if y == 0 { theme-blue } else if calc.odd(y) { rgb(248, 249, 250) } else { white },
  table.header(text(fill: white)[*Mode*], text(fill: white)[*Description*]),
  [*Beeps*], [Short beep tones for events (default)],
  [*Voice (TTS)*], [Spoken announcements via Edge TTS],
  [*Silent*], [No audio feedback],
)

== TTS Announcements

When Voice mode is enabled, you'll hear spoken feedback:

#table(
  columns: (auto, auto),
  inset: 10pt,
  fill: (x, y) => if y == 0 { theme-blue } else if calc.odd(y) { rgb(248, 249, 250) } else { white },
  table.header(text(fill: white)[*Event*], text(fill: white)[*Announcement*]),
  [Recording started], ["Recording"],
  [Recording stopped], ["Recording stopped"],
  [Audio cached], ["Cached"],
  [Transcription started], ["Transcribing"],
  [Transcription complete], ["Complete"],
  [Text copied], ["Text on clipboard"],
  [Recording cleared], ["Recording discarded"],
  [Error occurred], ["Error"],
)

TTS uses pre-generated audio files with a British English male voice (en-GB-RyanNeural via Microsoft Edge TTS).

= Transcript History

All transcriptions are automatically saved to a local MongoDB-compatible database.

== Database

Voice Notepad uses *Mongita*, a pure Python MongoDB implementation. Data is stored locally at `~/.config/voice-notepad-v3/mongita/`.

Each transcript record includes:
- Full transcript text
- Timestamp
- Provider and model used
- Audio duration (original and after VAD)
- Inference time
- Token usage and cost
- Word and character counts

== History Tab Features

- *Full-text search* across all transcriptions
- *Click to preview* any transcript
- *Double-click to load* into editor for reuse
- *Delete individual* transcriptions
- *Delete all* with confirmation

= Analytics & Cost Tracking

== Analytics Tab

The Analytics tab provides performance insights:

- *Summary statistics*: Total transcriptions, words, characters
- *Daily activity chart*: Visual bar chart with metric toggles
- *Model performance table*: Compare inference times across models
- *Time period selector*: Today, 7 days, 30 days, or all time

== Export Statistics

Click *Export Stats* to save anonymized statistics as JSON. Exported data includes:
- Transcription counts and volume
- Model performance metrics (no transcript content)
- Daily activity breakdown

Useful for benchmarking or sharing performance data.

== Cost Tab

For OpenRouter users, the Cost tab shows:

- *Account balance*: Live credit balance
- *Key-specific usage*: Daily, weekly, monthly spend
- *Model breakdown*: Usage by model

== Cost Effectiveness

Real usage data with Gemini 2.5 Flash:
- 848 transcriptions for \$1.17 total
- 84,000 words transcribed and cleaned
- About \$0.014 per 1,000 words (1.4 cents)

= Text Injection (Wayland)

Text injection automatically pastes transcribed text at your cursor after transcription.

== Requirements

- `ydotool` package installed
- `ydotoold` daemon running as your user (not root)

== Quick Setup

```bash
sudo apt install ydotool
sudo pkill ydotoold
sudo rm -f /tmp/.ydotool_socket
ydotoold &
```

== Verify

```bash
ls -la /tmp/.ydotool_socket
# Should show YOUR username as owner, not root
```

== Persistent Setup

Create `~/.config/systemd/user/ydotoold.service`:

```ini
[Unit]
Description=ydotool daemon

[Service]
ExecStart=/usr/bin/ydotoold

[Install]
WantedBy=default.target
```

Enable: `systemctl --user enable --now ydotoold`

= Output Modes

Three independent output modes can be combined:

#table(
  columns: (auto, 1fr),
  inset: 10pt,
  fill: (x, y) => if y == 0 { theme-blue } else if calc.odd(y) { rgb(248, 249, 250) } else { white },
  table.header(text(fill: white)[*Mode*], text(fill: white)[*Behavior*]),
  [*App*], [Text appears in the application window],
  [*Clipboard*], [Text copied to system clipboard],
  [*Inject*], [Text typed at cursor via ydotool],
)

Enable any combination: all three, any two, or just one.

= Storage Locations

All data is stored in `~/.config/voice-notepad-v3/`:

#table(
  columns: (auto, 1fr),
  inset: 10pt,
  fill: (x, y) => if y == 0 { theme-blue } else if calc.odd(y) { rgb(248, 249, 250) } else { white },
  table.header(text(fill: white)[*Path*], text(fill: white)[*Contents*]),
  [`config.json`], [API keys and preferences],
  [`mongita/`], [MongoDB-compatible transcript database],
  [`usage/`], [Daily cost tracking JSON files],
  [`audio-archive/`], [Opus audio recordings (if enabled)],
)

= Troubleshooting

== Audio Issues

*No microphone detected*: Check `pactl list sources short`. Verify PipeWire is running.

*Poor quality*: Enable AGC in Settings → Behavior. Position microphone closer.

== API Issues

*Transcription fails*: Verify API key. Check internet. Try different model.

*High costs*: Enable VAD. Use Flash Lite models.

== Hotkeys Not Working

Add yourself to input group: `sudo usermod -aG input $USER` then log out/in.

== Text Injection Not Working

Verify ydotoold is running as your user, not root. Check socket ownership.

= Support

Voice Notepad is open source software provided on a *best-effort basis*. No formal support is provided.

- *Repository*: #link("https://github.com/danielrosehill/Voice-Notepad")
- *Documentation*: #link("https://github.com/danielrosehill/Voice-Notepad/tree/main/docs")

Bug reports and feature requests can be submitted via GitHub Issues, but responses are not guaranteed.

#v(2cm)
#align(center)[
  #line(length: 50%, stroke: 0.5pt + rgb(204, 204, 204))
  #v(0.5cm)
  #text(10pt, fill: rgb(102, 102, 102))[
    Voice Notepad User Manual v3 \
    Version 1.9.11 · December 2025 \
    Created by Daniel Rosehill with Claude (Anthropic) \
    MIT License
  ]
]
